{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3fccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa03ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "# TODO: add EDA4\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['1_tiny', '2_small', '3_standard', '4_full']\n",
    "\n",
    "size_folders = ['size_data_f1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'size_data_f1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'size_data_f1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'size_data_f1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'size_data_f1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "if not os.path.isdir('size_data_f1'):\n",
    "    os.mkdir('size_data_f1')\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "#alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "alphas = [0.05, 0.1, 0.2, 0.4]\n",
    "#alphas = [0.05, 0.15, 0.35, 0.5]\n",
    "\n",
    "# Number of words for input\n",
    "input_size_list = [50,50,40,25,25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10774ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_cnn(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "    test_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=10, \n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=0)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y, test_x, test_y, model = None, None, None, None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1941e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_model(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "    test_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=10, \n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=0)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y, test_x, test_y, model = None, None, None, None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "410be237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn weda aug  0.7392795883361921 0.05 size_data_f1/1_tiny/sst2\n",
      "cnn eda aug 0.7438536306460835\n",
      "cnn weda aug  0.7455688965122927 0.1 size_data_f1/1_tiny/sst2\n",
      "cnn eda aug 0.7421383647798742\n",
      "cnn weda aug  0.7387078330474557 0.2 size_data_f1/1_tiny/sst2\n",
      "cnn eda aug 0.7501429388221841\n",
      "cnn weda aug  0.7392795883361921 0.4 size_data_f1/1_tiny/sst2\n",
      "cnn eda aug 0.7489994282447112\n",
      "Performances WEDA\n",
      "0.05 : 0.7392795883361921\n",
      "0.1 : 0.7455688965122927\n",
      "0.2 : 0.7387078330474557\n",
      "0.4 : 0.7392795883361921\n",
      "Performances EDA\n",
      "0.05 : 0.7438536306460835\n",
      "0.1 : 0.7421383647798742\n",
      "0.2 : 0.7501429388221841\n",
      "0.4 : 0.7489994282447112\n",
      "{0.05: [0.7392795883361921], 0.1: [0.7455688965122927], 0.2: [0.7387078330474557], 0.4: [0.7392795883361921]}\n",
      "{0.05: [0.7438536306460835], 0.1: [0.7421383647798742], 0.2: [0.7501429388221841], 0.4: [0.7489994282447112]}\n",
      "cnn weda aug  0.7970268724985706 0.05 size_data_f1/2_small/sst2\n",
      "cnn eda aug 0.8021726700971984\n",
      "cnn weda aug  0.8044596912521441 0.1 size_data_f1/2_small/sst2\n",
      "cnn eda aug 0.7941680960548885\n",
      "cnn weda aug  0.7953116066323613 0.2 size_data_f1/2_small/sst2\n",
      "cnn eda aug 0.7895940537449971\n",
      "cnn weda aug  0.8078902229845626 0.4 size_data_f1/2_small/sst2\n",
      "cnn eda aug 0.7981703830760435\n",
      "Performances WEDA\n",
      "0.05 : 0.7970268724985706\n",
      "0.1 : 0.8044596912521441\n",
      "0.2 : 0.7953116066323613\n",
      "0.4 : 0.8078902229845626\n",
      "Performances EDA\n",
      "0.05 : 0.8021726700971984\n",
      "0.1 : 0.7941680960548885\n",
      "0.2 : 0.7895940537449971\n",
      "0.4 : 0.7981703830760435\n",
      "{0.05: [0.7970268724985706], 0.1: [0.8044596912521441], 0.2: [0.7953116066323613], 0.4: [0.8078902229845626]}\n",
      "{0.05: [0.8021726700971984], 0.1: [0.7941680960548885], 0.2: [0.7895940537449971], 0.4: [0.7981703830760435]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75b015e019d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train_weda_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'size_data_f1/test/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn weda aug \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-78ada428cec9>\u001b[0m in \u001b[0;36mrun_cnn\u001b[0;34m(train_file, test_file, num_classes, percent_dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 verbose=0)\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#model.save('checkpoints/lol')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#model = load_model('checkpoints/lol')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for each method\n",
    "## TODO: add original EDA\n",
    "writer_cnn = open('outputs_f1/cnn_general'+'_' + get_now_str() + '.txt', 'w')\n",
    "writer_rnn = open('outputs_f1/rnn_general' + '_' + get_now_str() + '.txt', 'w')\n",
    "\n",
    "\n",
    "#for each size dataset\n",
    "for size_folder in size_folders:\n",
    "\n",
    "    writer_cnn.write(size_folder + '\\n')\n",
    "    writer_rnn.write(size_folder + '\\n')\n",
    "\n",
    "    #get all six datasets\n",
    "    dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "\n",
    "    #for storing the performances\n",
    "    performances_weda = {alpha:[] for alpha in alphas}\n",
    "    performances_eda = {alpha:[] for alpha in alphas}\n",
    "\n",
    "    #for each dataset\n",
    "    for i in range(len(dataset_folders)):\n",
    "\n",
    "        #initialize all the variables\n",
    "        dataset_folder = dataset_folders[i]\n",
    "        dataset = datasets[i]\n",
    "        num_classes = num_classes_list[i]\n",
    "        input_size = input_size_list[i]\n",
    "        word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "        word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "        #test each alpha value\n",
    "        for alpha in alphas:\n",
    "\n",
    "            train_path = dataset_folder + '/train_weda_' + str(alpha) + '.txt'\n",
    "            test_path = 'size_data_f1/test/' + dataset + '/test.txt'\n",
    "            acc = run_cnn(train_path, test_path, num_classes, percent_dataset=1)\n",
    "            print(\"cnn weda aug \", acc, alpha, dataset_folder)\n",
    "\n",
    "            performances_weda[alpha].append(acc)\n",
    "        \n",
    "            train_path = dataset_folder + '/train_eda_' + str(alpha) + '.txt'\n",
    "            acc = run_cnn(train_path, test_path, num_classes, percent_dataset=1)\n",
    "            performances_eda[alpha].append(acc)\n",
    "            print(\"cnn eda aug\", acc)\n",
    "            \n",
    "\n",
    "    writer_cnn.write(str(performances_weda) + '\\n')\n",
    "    writer_rnn.write(str(performances_eda)+\"\\n\")\n",
    "    print(\"Performances WEDA\")\n",
    "    for alpha in performances_weda:\n",
    "        line = str(alpha) + ' : ' + str(sum(performances_weda[alpha])/len(performances_weda[alpha]))\n",
    "        writer_cnn.write(line + '\\n')\n",
    "        print(line)\n",
    "    \n",
    "    print(\"Performances EDA\")\n",
    "    for alpha in performances_eda:\n",
    "        line = str(alpha) + ' : ' + str(sum(performances_eda[alpha])/len(performances_eda[alpha]))\n",
    "        writer_rnn.write(line + '\\n')\n",
    "        print(line)\n",
    "    \n",
    "    print(performances_weda)\n",
    "    print(performances_eda)\n",
    "\n",
    "writer_cnn.close()\n",
    "writer_rnn.close()\n",
    "    \n",
    "## TODO: add training without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer_cnn = open('outputs_f1/cnn_no_aug_' + get_now_str() + '.txt', 'w')\n",
    "writer_rnn = open('outputs_f1/rnn_no_aug_' + get_now_str() + '.txt', 'w')\n",
    "\n",
    "\n",
    "#for each size dataset\n",
    "for size_folder in size_folders:\n",
    "\n",
    "    writer_cnn.write(size_folder + '\\n')\n",
    "    writer_rnn.write(size_folder + '\\n')\n",
    "\n",
    "    #get all six datasets\n",
    "    dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "\n",
    "    #for storing the performances\n",
    "    performances_rnn = []\n",
    "    performances_cnn = []\n",
    "\n",
    "    #for each dataset\n",
    "    for i in range(len(dataset_folders)):\n",
    "\n",
    "        #initialize all the variables\n",
    "        dataset_folder = dataset_folders[i]\n",
    "        dataset = datasets[i]\n",
    "        num_classes = num_classes_list[i]\n",
    "        input_size = input_size_list[i]\n",
    "        word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "        word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "        train_path = dataset_folder + '/train_orig.txt'\n",
    "        test_path = 'size_data_f1/test/' + dataset + '/test.txt'\n",
    "        \n",
    "        acc = run_cnn(train_path, test_path, num_classes, percent_dataset=1)\n",
    "        print(\"orignal no aug\", acc)\n",
    "        performances_cnn.append(acc)\n",
    "\n",
    "        \"\"\"\n",
    "        acc = run_rnn(train_path, test_path, num_classes, percent_dataset=1)\n",
    "        performances_rnn.append(acc)\n",
    "        print(\"rnn\", acc)\n",
    "        \"\"\"\n",
    "\n",
    "    writer_cnn.write(str(performances_cnn) + '\\n')\n",
    "    writer_rnn.write(str(performances_rnn)+\"\\n\")\n",
    "\n",
    "    print(performances_cnn)\n",
    "    print(performances_rnn)\n",
    "\n",
    "writer_cnn.close()\n",
    "writer_rnn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da613fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
