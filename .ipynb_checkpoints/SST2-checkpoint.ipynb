{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf16a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken and adapted from : https://github.com/jasonwei20/eda_nlp/tree/master/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2aec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa1a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## config file\n",
    "datasets = ['sst']\n",
    "dataset_folders = ['increment_datasets_f2/' + dataset for dataset in datasets] \n",
    "num_classes_list = [2]\n",
    "input_size_list = [50]\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300\n",
    "increments = [0.7, 0.8, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fd8cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished eda for sst/train_orig.txt to sst/train_aug_st.txt\n",
      "['sst/train_aug_st.txt', 'sst/train_orig.txt']\n",
      "73655 unique words found\n",
      "31950 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to sst/word2vec.p\n"
     ]
    }
   ],
   "source": [
    "#pre-existing file locations\n",
    "dataset_folder = \"sst\"\n",
    "\n",
    "train_orig = dataset_folder + '/train_orig.txt'\n",
    "\n",
    "#file to be created\n",
    "train_aug_st = dataset_folder + '/train_aug_st.txt'\n",
    "\n",
    "#standard augmentation\n",
    "gen_standard_aug(train_orig, train_aug_st)\n",
    "\n",
    "#generate the vocab dictionary\n",
    "word2vec_pickle = dataset_folder + '/word2vec.p' # don't want to load the huge pickle every time, so just save the words that are actually used into a smaller dictionary\n",
    "gen_vocab_dicts(dataset_folder, word2vec_pickle, huge_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982a1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#### run model and get acc ####\n",
    "###############################\n",
    "\n",
    "def run_model(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_model(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "    test_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=100000, \n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=0)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y = None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f273e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10831 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-af2cf18fc760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#calculate augmented accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0maug_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_aug_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0maug_accs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a54b0de9f91c>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(train_file, test_file, num_classes, percent_dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP project/methods.py\u001b[0m in \u001b[0;36mget_x_y\u001b[0;34m(train_txt, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m#insert y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0my_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10831 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "#get the accuracy at each increment\n",
    "## TODO: there is some problemo , fix first the directories problem, coming from first cells, then verify we are properly using the correct dataset and identify where the problem is coming from\n",
    "\n",
    "orig_accs = {dataset:{} for dataset in datasets}\n",
    "aug_accs = {dataset:{} for dataset in datasets}\n",
    "\n",
    "writer = open('outputs_f2/' + get_now_str() + '.csv', 'w')\n",
    "\n",
    "#for each dataset\n",
    "for i, dataset_folder in enumerate(dataset_folders):\n",
    "\n",
    "    dataset = datasets[i]\n",
    "    num_classes = num_classes_list[i]\n",
    "    input_size = input_size_list[i]\n",
    "    train_orig = 'sst/train_orig.txt' #dataset_folder + '/train_orig.txt'\n",
    "    train_aug_st = 'sst/train_aug_st.txt' #dataset_folder + '/train_aug_st.txt'\n",
    "    test_path = 'sst/test.txt'#dataset_folder + '/test.txt'\n",
    "    word2vec_pickle = 'sst/word2vec.p' ## dataset_folder + '/word2vec.p' TODO: check this\n",
    "    word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "    for increment in increments:\n",
    "\n",
    "        #calculate augmented accuracy\n",
    "        aug_acc = run_model(train_aug_st, test_path, num_classes, increment)\n",
    "        aug_accs[dataset][increment] = aug_acc\n",
    "\n",
    "        #calculate original accuracy\n",
    "        orig_acc = run_model(train_orig, test_path, num_classes, increment)\n",
    "        orig_accs[dataset][increment] = orig_acc\n",
    "\n",
    "        print(dataset, increment, orig_acc, aug_acc)\n",
    "        writer.write(dataset + ',' + str(increment) + ',' + str(orig_acc) + ',' + str(aug_acc) + '\\n')\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "print(orig_accs, aug_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
