{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447f1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3207da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['1_tiny', '2_small', '3_standard', '4_full']\n",
    "size_folders = ['size_data_f1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'size_data_f1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'size_data_f1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'size_data_f1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'size_data_f1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "a_methods = [\"sr\", \"ri\", \"rd\", \"rs\"]\n",
    "\n",
    "# Number of words for input\n",
    "input_size_list = [50,50,40,25,25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c755ee6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'size_data_f1/1_tiny/sst2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-95ab6b440855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_folders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mn_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_aug_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'size_data_f1/1_tiny/sst2'"
     ]
    }
   ],
   "source": [
    "for a_method in a_methods:\n",
    "\n",
    "    for size_folder in size_folders:\n",
    "        if not os.path.isdir(size_folder):\n",
    "            os.path.mkdir(size_folder)\n",
    "            \n",
    "        dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "        n_aug_list = n_aug_list_dict[size_folder]\n",
    "\n",
    "        #for each dataset\n",
    "        for i, dataset_folder in enumerate(dataset_folders):\n",
    "            if not os.path.isdir(dataset_folder):\n",
    "                os.mkdir(dataset_folder)\n",
    "                \n",
    "            n_aug = n_aug_list[i]\n",
    "\n",
    "            #pre-existing file locations\n",
    "            train_orig = dataset_folder + '/train_orig.txt'\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                output_file = dataset_folder + '/train_'+a_method+\"_\"+str(alpha)+'.txt'\n",
    "                \n",
    "                # generate augmented data\n",
    "                if a_method == \"sr\":\n",
    "                    gen_sr_aug(train_orig, output_file, alpha, n_aug)\n",
    "                if a_method == \"ri\":\n",
    "                    gen_ri_aug(train_orig, output_file, alpha, n_aug)\n",
    "                if a_method == \"rd\":\n",
    "                    gen_rd_aug(train_orig, output_file, alpha, n_aug)\n",
    "                if a_method == \"rs\":\n",
    "                    gen_rs_aug(train_orig, output_file, alpha, n_aug)\n",
    "                    \n",
    "\n",
    "            #generate the vocab dictionary\n",
    "            word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "            gen_vocab_dicts(dataset_folder, word2vec_pickle, huge_word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
