{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c092135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e26854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['1_tiny', '2_small', '3_standard', '4_full']\n",
    "size_folders = ['alphas_sized_decomposed_f1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'alphas_sized_decomposed_f1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'alphas_sized_decomposed_f1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'alphas_sized_decomposed_f1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'alphas_sized_decomposed_f1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "if not os.path.isdir('alphas_sized_decomposed_f1'):\n",
    "    os.mkdir('alphas_sized_decomposed_f1')\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "#alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "alphas = [0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "#a_methods = [\"rd\", \"ri\", \"sr\", \"rs\"]\n",
    "a_methods = [\"rd\"]\n",
    "#a_methods = [\"sr\", \"ri\", \"rs\"]\n",
    "\n",
    "\n",
    "# Number of words for input\n",
    "input_size_list = [50,50,40,25,25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb5256",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.05.txt with alpha 0.05\n",
      "EDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.05.txt with alpha 0.05\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.1.txt with alpha 0.1\n",
      "EDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.1.txt with alpha 0.1\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.2.txt with alpha 0.2\n",
      "EDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.2.txt with alpha 0.2\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.4.txt with alpha 0.4\n",
      "EDA finished RD for alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt to alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.4.txt with alpha 0.4\n",
      "['alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rd_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_ri_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_ri_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_ri_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_ri_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rs_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rs_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rs_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_rs_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_sr_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_sr_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_sr_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_eda_sr_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_orig.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rd_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_ri_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_ri_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_ri_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_ri_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rs_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rs_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rs_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_rs_0.4.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_sr_0.05.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_sr_0.1.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_sr_0.2.txt', 'alphas_sized_decomposed_f1/1_tiny/sst2/train_weda_sr_0.4.txt']\n",
      "17687 unique words found\n",
      "11298 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to alphas_sized_decomposed_f1/1_tiny/sst2/word2vec.p\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.05.txt with alpha 0.05\n",
      "EDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.05.txt with alpha 0.05\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.1.txt with alpha 0.1\n",
      "EDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.1.txt with alpha 0.1\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.2.txt with alpha 0.2\n",
      "EDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.2.txt with alpha 0.2\n",
      "WEDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.4.txt with alpha 0.4\n",
      "EDA finished RD for alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt to alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.4.txt with alpha 0.4\n",
      "['alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rd_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_ri_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_ri_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_ri_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_ri_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rs_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rs_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rs_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_rs_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_sr_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_sr_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_sr_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_eda_sr_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_orig.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rd_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_ri_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_ri_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_ri_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_ri_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rs_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rs_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rs_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_rs_0.4.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_sr_0.05.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_sr_0.1.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_sr_0.2.txt', 'alphas_sized_decomposed_f1/2_small/sst2/train_weda_sr_0.4.txt']\n",
      "34205 unique words found\n"
     ]
    }
   ],
   "source": [
    "for a_method in a_methods:\n",
    "\n",
    "    for size_folder in size_folders:\n",
    "        if not os.path.isdir(size_folder):\n",
    "            os.mkdir(size_folder)\n",
    "            \n",
    "        dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "        n_aug_list = n_aug_list_dict[size_folder]\n",
    "\n",
    "        #for each dataset\n",
    "        for i, dataset_folder in enumerate(dataset_folders):\n",
    "            if not os.path.isdir(dataset_folder):\n",
    "                os.mkdir(dataset_folder)\n",
    "                \n",
    "            n_aug = n_aug_list[i]\n",
    "\n",
    "            #pre-existing file locations\n",
    "            train_orig = dataset_folder + '/train_orig.txt'\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                output_file_weda = dataset_folder + '/train_weda_'+a_method+\"_\"+str(alpha)+'.txt'\n",
    "                output_file_eda = dataset_folder + '/train_eda_'+a_method+\"_\"+str(alpha)+'.txt'\n",
    "                \n",
    "                # generate augmented data\n",
    "                if a_method == \"sr\":\n",
    "                    gen_sr_aug_weda(train_orig, output_file_weda, alpha, n_aug)\n",
    "                    gen_sr_aug_eda(train_orig, output_file_eda, alpha, n_aug)\n",
    "                if a_method == \"ri\":\n",
    "                    gen_ri_aug_weda(train_orig, output_file_weda, alpha, n_aug)\n",
    "                    gen_ri_aug_eda(train_orig, output_file_eda, alpha, n_aug)\n",
    "                if a_method == \"rd\":\n",
    "                    gen_rd_aug_weda(train_orig, output_file_weda, alpha, n_aug)\n",
    "                    gen_rd_aug_eda(train_orig, output_file_eda, alpha, n_aug)\n",
    "                if a_method == \"rs\":\n",
    "                    gen_rs_aug_weda(train_orig, output_file_weda, alpha, n_aug)\n",
    "                    gen_rs_aug_eda(train_orig, output_file_eda, alpha, n_aug)\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "            #generate the vocab dictionary\n",
    "            word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "            gen_vocab_dicts(dataset_folder, word2vec_pickle, huge_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1d84a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas_sized_decomposed_f1/test/sst2/test.txt\n"
     ]
    }
   ],
   "source": [
    "#for each dataset\n",
    "for s in enumerate(datasets):\n",
    "    dataset = datasets[i]\n",
    "    #pre-existing file locations\n",
    "    test_orig = dataset + '/test.txt'\n",
    "    folder = \"alphas_sized_decomposed_f1/test/\" + dataset \n",
    "    \n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "        \n",
    "    output_file = folder+  '/test.txt'\n",
    "    print(output_file)\n",
    "    shutil.copy(test_orig, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
