{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf16a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken and adapted from : https://github.com/jasonwei20/eda_nlp/tree/master/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2aec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa1a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## config file\n",
    "datasets = ['sst2']\n",
    "dataset_folders = ['increment_datasets_f2/' + dataset for dataset in datasets] \n",
    "num_classes_list = [2]\n",
    "input_size_list = [50]\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300\n",
    "increments = [0.7, 0.8, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45fd8cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished eda for sst2/train_orig.txt to sst2/train_aug_st.txt\n",
      "['sst2/test.txt', 'sst2/train_aug_st.txt', 'sst2/train_orig.txt']\n",
      "30289 unique words found\n",
      "23189 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to sst2/word2vec.p\n"
     ]
    }
   ],
   "source": [
    "#pre-existing file locations\n",
    "dataset_folder = \"sst2\"\n",
    "\n",
    "train_orig = dataset_folder + '/train_orig.txt'\n",
    "\n",
    "#file to be created\n",
    "train_aug_st = dataset_folder + '/train_aug_st.txt'\n",
    "\n",
    "#standard augmentation\n",
    "gen_standard_aug(train_orig, train_aug_st)\n",
    "\n",
    "#generate the vocab dictionary\n",
    "word2vec_pickle = dataset_folder + '/word2vec.p' # don't want to load the huge pickle every time, so just save the words that are actually used into a smaller dictionary\n",
    "gen_vocab_dicts(dataset_folder, word2vec_pickle, huge_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982a1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#### run model and get acc ####\n",
    "###############################\n",
    "\n",
    "def run_model(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_model(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "    test_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "    print(train_x[:3])\n",
    "    print(train_y[:3])\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=20, # TODO they had 100000\n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=3)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    print(y_pred)\n",
    "    print(test_y)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y = None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f273e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.12671    -0.21656001 -0.025641   ...  0.52364999 -0.037669\n",
      "   -0.43900999]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.014719    0.38536    -0.066643   ...  1.43869996 -0.38578999\n",
      "    0.38552999]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.089187    0.25792     0.26282001 ...  0.14421    -0.169\n",
      "    0.26501   ]\n",
      "  [-0.087595    0.35501999  0.063868   ...  0.03446    -0.15027\n",
      "    0.40673   ]\n",
      "  [-0.24931     0.46448001 -0.31274    ... -0.12095     0.03795\n",
      "    0.0020277 ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.18732999  0.40595001 -0.51174003 ...  0.16495     0.18757001\n",
      "    0.53873998]\n",
      "  [ 0.13879     0.64071    -0.51120001 ... -0.10391    -0.10502\n",
      "   -0.24558   ]\n",
      "  [-0.04983     0.02705    -0.38787001 ... -0.30357999  0.27074999\n",
      "    0.18046001]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "[[3.5067227e-02 9.6493274e-01]\n",
      " [9.9998713e-01 1.2827302e-05]\n",
      " [8.2783476e-03 9.9172157e-01]\n",
      " ...\n",
      " [9.9984932e-01 1.5069796e-04]\n",
      " [9.9989355e-01 1.0649131e-04]\n",
      " [9.9967635e-01 3.2361908e-04]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [-4.83609997e-02  2.29809999e-01  3.16910003e-03 ... -3.46329987e-01\n",
      "    2.39640009e-02 -2.64189988e-01]\n",
      "  [-4.84290004e-01  6.77720010e-01  6.51880026e-01 ... -4.65730011e-01\n",
      "   -2.20129993e-02  5.45700006e-02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[-1.71580002e-01  4.04890001e-01 -2.94800013e-01 ...  1.18799999e-01\n",
      "    2.44810000e-01  9.30079967e-02]\n",
      "  [-2.79520005e-01  2.56449997e-01 -2.69540008e-02 ... -4.37460005e-01\n",
      "    2.68839985e-01 -1.17990002e-01]\n",
      "  [ 8.55199993e-02  5.01519978e-01  1.12659998e-01 ... -1.55080006e-01\n",
      "    4.22689989e-02  1.44900000e-02]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[-2.09959999e-01 -1.55770004e-01 -3.77359986e-01 ... -5.56729972e-01\n",
      "    7.27760023e-04  6.50549978e-02]\n",
      "  [-2.34750003e-01 -1.63640000e-03 -9.49599966e-02 ...  6.04089983e-02\n",
      "   -4.05180007e-01 -5.44349998e-02]\n",
      "  [ 8.78830031e-02  7.17689991e-02  8.13430011e-01 ...  1.67339996e-01\n",
      "   -1.78140000e-01  3.58619988e-01]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "[[0.54876614 0.4512339 ]\n",
      " [0.5654031  0.43459684]\n",
      " [0.5499753  0.45002478]\n",
      " ...\n",
      " [0.55885446 0.44114548]\n",
      " [0.564931   0.435069  ]\n",
      " [0.5575831  0.44241688]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "sst2 0.7 0.5669683257918552 0.5380090497737556\n",
      "{'sst2': {0.7: 0.5669683257918552}} {'sst2': {0.7: 0.5380090497737556}}\n"
     ]
    }
   ],
   "source": [
    "#get the accuracy at each increment\n",
    "\n",
    "orig_accs = {dataset:{} for dataset in datasets}\n",
    "aug_accs = {dataset:{} for dataset in datasets}\n",
    "\n",
    "writer = open('outputs_f2/' + get_now_str() + '.csv', 'w')\n",
    "\n",
    "#for each dataset\n",
    "for i, dataset_folder in enumerate(dataset_folders):\n",
    "\n",
    "    dataset = datasets[i]\n",
    "    num_classes = num_classes_list[i]\n",
    "    input_size = input_size_list[i]\n",
    "    train_orig = 'sst/train_orig.txt' #dataset_folder + '/train_orig.txt'\n",
    "    train_aug_st = 'sst/train_aug_st.txt' #dataset_folder + '/train_aug_st.txt'\n",
    "    test_path = 'sst/test.txt'#dataset_folder + '/test.txt'\n",
    "    word2vec_pickle = 'sst/word2vec.p' ## dataset_folder + '/word2vec.p' TODO: check this\n",
    "    word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "    #for increment in increments:\n",
    "    increment = 0.7\n",
    "    #calculate augmented accuracy\n",
    "    aug_acc = run_model(train_aug_st, test_path, num_classes, increment)\n",
    "    aug_accs[dataset][increment] = aug_acc\n",
    "\n",
    "    #calculate original accuracy\n",
    "    orig_acc = run_model(train_orig, test_path, num_classes, increment)\n",
    "    orig_accs[dataset][increment] = orig_acc\n",
    "\n",
    "    print(dataset, increment, orig_acc, aug_acc)\n",
    "    writer.write(dataset + ',' + str(increment) + ',' + str(orig_acc) + ',' + str(aug_acc) + '\\n')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(orig_accs, aug_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2b4036",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-421157ec4fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400430a",
   "metadata": {},
   "source": [
    "# Custom test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3916cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_and_y(file, num_classes, word2vec_len, input_size, word2vec):\n",
    "    train_lines = open(file, 'r').readlines()\n",
    "    shuffle(train_lines)\n",
    "    train_lines = train_lines\n",
    "    num_lines = len(train_lines)\n",
    "\n",
    "    #initialize x and y matrix\n",
    "    x_matrix = None\n",
    "    y_matrix = None\n",
    "\n",
    "    try:\n",
    "        x_matrix = np.zeros((num_lines, input_size, word2vec_len))\n",
    "    except:\n",
    "        print(\"Error!\", num_lines, input_size, word2vec_len)\n",
    "    y_matrix = np.zeros((num_lines, num_classes))\n",
    "\n",
    "    #insert values\n",
    "    for i, line in enumerate(train_lines):\n",
    "\n",
    "        parts = line[:-1].split('\\t')\n",
    "        label = int(parts[0])\n",
    "        sentence = parts[1]\t\n",
    "        #insert x\n",
    "        words = sentence.split(' ')\n",
    "        words = words[:x_matrix.shape[1]] #cut off if too long\n",
    "        for j, word in enumerate(words):\n",
    "            if word in word2vec:\n",
    "                x_matrix[i, j, :] = word2vec[word]\n",
    "        #insert y\n",
    "        y_matrix[i][label] = 1.0\n",
    "\n",
    "    return x_matrix, y_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_model(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_and_y(train_file, num_classes, word2vec_len, input_size, word2vec)\n",
    "    test_x, test_y = get_x_and_y(test_file, num_classes, word2vec_len, input_size, word2vec)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "    print(train_x[:3])\n",
    "    print(train_y[:3])\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=2, # TODO they had 100000\n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=3)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    print(y_pred)\n",
    "    print(test_y)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y = None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
