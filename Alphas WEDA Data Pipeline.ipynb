{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c3bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bfdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['1_tiny', '2_small', '3_standard', '4_full']\n",
    "size_folders = ['alphas_sized_f1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'alphas_sized_f1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'alphas_sized_f1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'alphas_sized_f1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'alphas_sized_f1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "if not os.path.isdir('alphas_sized_f1'):\n",
    "    os.mkdir('alphas_sized_f1')\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "#alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "alphas = [0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "# Number of words for input\n",
    "input_size_list = [50,50,40,25,25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84d7373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished weda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_weda_0.05.txt\n",
      "finished eda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_eda_0.05.txt\n",
      "finished weda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_weda_0.1.txt\n",
      "finished eda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_eda_0.1.txt\n",
      "finished weda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_weda_0.2.txt\n",
      "finished eda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_eda_0.2.txt\n",
      "finished weda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_weda_0.4.txt\n",
      "finished eda for alphas_sized_f1/1_tiny/sst2/train_orig.txt to alphas_sized_f1/1_tiny/sst2/train_eda_0.4.txt\n",
      "['alphas_sized_f1/1_tiny/sst2/train_eda_0.05.txt', 'alphas_sized_f1/1_tiny/sst2/train_eda_0.1.txt', 'alphas_sized_f1/1_tiny/sst2/train_eda_0.2.txt', 'alphas_sized_f1/1_tiny/sst2/train_eda_0.4.txt', 'alphas_sized_f1/1_tiny/sst2/train_orig.txt', 'alphas_sized_f1/1_tiny/sst2/train_weda_0.05.txt', 'alphas_sized_f1/1_tiny/sst2/train_weda_0.1.txt', 'alphas_sized_f1/1_tiny/sst2/train_weda_0.2.txt', 'alphas_sized_f1/1_tiny/sst2/train_weda_0.3.txt', 'alphas_sized_f1/1_tiny/sst2/train_weda_0.4.txt', 'alphas_sized_f1/1_tiny/sst2/train_weda_0.5.txt']\n",
      "16014 unique words found\n",
      "10909 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to alphas_sized_f1/1_tiny/sst2/word2vec.p\n",
      "finished weda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_weda_0.05.txt\n",
      "finished eda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_eda_0.05.txt\n",
      "finished weda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_weda_0.1.txt\n",
      "finished eda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_eda_0.1.txt\n",
      "finished weda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_weda_0.2.txt\n",
      "finished eda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_eda_0.2.txt\n",
      "finished weda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_weda_0.4.txt\n",
      "finished eda for alphas_sized_f1/2_small/sst2/train_orig.txt to alphas_sized_f1/2_small/sst2/train_eda_0.4.txt\n",
      "['alphas_sized_f1/2_small/sst2/train_eda_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_eda_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_eda_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_eda_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_eda_rd_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_eda_rd_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_eda_rd_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_eda_rd_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_orig.txt', 'alphas_sized_f1/2_small/sst2/train_rd_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_rd_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_rd_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_rd_0.3.txt', 'alphas_sized_f1/2_small/sst2/train_rd_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_rd_0.5.txt', 'alphas_sized_f1/2_small/sst2/train_ri_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_ri_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_ri_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_ri_0.3.txt', 'alphas_sized_f1/2_small/sst2/train_ri_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_ri_0.5.txt', 'alphas_sized_f1/2_small/sst2/train_rs_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_rs_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_rs_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_rs_0.3.txt', 'alphas_sized_f1/2_small/sst2/train_rs_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_rs_0.5.txt', 'alphas_sized_f1/2_small/sst2/train_sr_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_sr_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_sr_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_sr_0.3.txt', 'alphas_sized_f1/2_small/sst2/train_sr_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_sr_0.5.txt', 'alphas_sized_f1/2_small/sst2/train_weda_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_weda_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_weda_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_weda_0.3.txt', 'alphas_sized_f1/2_small/sst2/train_weda_0.4.txt', 'alphas_sized_f1/2_small/sst2/train_weda_0.5.txt', 'alphas_sized_f1/2_small/sst2/train_weda_rd_0.05.txt', 'alphas_sized_f1/2_small/sst2/train_weda_rd_0.1.txt', 'alphas_sized_f1/2_small/sst2/train_weda_rd_0.2.txt', 'alphas_sized_f1/2_small/sst2/train_weda_rd_0.4.txt']\n",
      "35656 unique words found\n",
      "18737 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to alphas_sized_f1/2_small/sst2/word2vec.p\n",
      "finished weda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_weda_0.05.txt\n",
      "finished eda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_eda_0.05.txt\n",
      "finished weda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_weda_0.1.txt\n",
      "finished eda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_eda_0.1.txt\n",
      "finished weda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_weda_0.2.txt\n",
      "finished eda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_eda_0.2.txt\n",
      "finished weda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_weda_0.4.txt\n",
      "finished eda for alphas_sized_f1/3_standard/sst2/train_orig.txt to alphas_sized_f1/3_standard/sst2/train_eda_0.4.txt\n",
      "['alphas_sized_f1/3_standard/sst2/train_eda_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_rd_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_rd_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_rd_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_eda_rd_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_orig.txt', 'alphas_sized_f1/3_standard/sst2/train_rd_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_rd_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_rd_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_rd_0.3.txt', 'alphas_sized_f1/3_standard/sst2/train_rd_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_rd_0.5.txt', 'alphas_sized_f1/3_standard/sst2/train_ri_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_ri_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_ri_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_ri_0.3.txt', 'alphas_sized_f1/3_standard/sst2/train_ri_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_ri_0.5.txt', 'alphas_sized_f1/3_standard/sst2/train_rs_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_rs_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_rs_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_rs_0.3.txt', 'alphas_sized_f1/3_standard/sst2/train_rs_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_rs_0.5.txt', 'alphas_sized_f1/3_standard/sst2/train_sr_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_sr_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_sr_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_sr_0.3.txt', 'alphas_sized_f1/3_standard/sst2/train_sr_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_sr_0.5.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_0.3.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_0.4.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_0.5.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_rd_0.05.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_rd_0.1.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_rd_0.2.txt', 'alphas_sized_f1/3_standard/sst2/train_weda_rd_0.4.txt']\n",
      "49443 unique words found\n",
      "24347 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to alphas_sized_f1/3_standard/sst2/word2vec.p\n",
      "finished weda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_weda_0.05.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished eda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_eda_0.05.txt\n",
      "finished weda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_weda_0.1.txt\n",
      "finished eda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_eda_0.1.txt\n",
      "finished weda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_weda_0.2.txt\n",
      "finished eda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_eda_0.2.txt\n",
      "finished weda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_weda_0.4.txt\n",
      "finished eda for alphas_sized_f1/4_full/sst2/train_orig.txt to alphas_sized_f1/4_full/sst2/train_eda_0.4.txt\n",
      "['alphas_sized_f1/4_full/sst2/train_eda_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_eda_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_eda_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_eda_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_eda_rd_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_eda_rd_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_eda_rd_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_eda_rd_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_orig.txt', 'alphas_sized_f1/4_full/sst2/train_rd_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_rd_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_rd_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_rd_0.3.txt', 'alphas_sized_f1/4_full/sst2/train_rd_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_rd_0.5.txt', 'alphas_sized_f1/4_full/sst2/train_ri_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_ri_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_ri_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_ri_0.3.txt', 'alphas_sized_f1/4_full/sst2/train_ri_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_ri_0.5.txt', 'alphas_sized_f1/4_full/sst2/train_rs_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_rs_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_rs_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_rs_0.3.txt', 'alphas_sized_f1/4_full/sst2/train_rs_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_rs_0.5.txt', 'alphas_sized_f1/4_full/sst2/train_sr_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_sr_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_sr_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_sr_0.3.txt', 'alphas_sized_f1/4_full/sst2/train_sr_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_sr_0.5.txt', 'alphas_sized_f1/4_full/sst2/train_weda_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_weda_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_weda_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_weda_0.3.txt', 'alphas_sized_f1/4_full/sst2/train_weda_0.4.txt', 'alphas_sized_f1/4_full/sst2/train_weda_0.5.txt', 'alphas_sized_f1/4_full/sst2/train_weda_rd_0.05.txt', 'alphas_sized_f1/4_full/sst2/train_weda_rd_0.1.txt', 'alphas_sized_f1/4_full/sst2/train_weda_rd_0.2.txt', 'alphas_sized_f1/4_full/sst2/train_weda_rd_0.4.txt']\n",
      "57548 unique words found\n",
      "27140 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to alphas_sized_f1/4_full/sst2/word2vec.p\n"
     ]
    }
   ],
   "source": [
    "for size_folder in size_folders:\n",
    "    if not os.path.isdir(size_folder):\n",
    "        os.mkdir(size_folder)\n",
    "\n",
    "    dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "    n_aug_list = n_aug_list_dict[size_folder]\n",
    "\n",
    "    #for each dataset\n",
    "    for i, dataset_folder in enumerate(dataset_folders):\n",
    "        if not os.path.isdir(dataset_folder):\n",
    "            os.mkdir(dataset_folder)\n",
    "\n",
    "        n_aug = n_aug_list[i]\n",
    "\n",
    "        #pre-existing file locations\n",
    "        train_orig = dataset_folder + '/train_orig.txt'\n",
    "\n",
    "        for alpha in alphas:\n",
    "            output_file_weda = dataset_folder + '/train_weda_'+str(alpha)+'.txt'\n",
    "\n",
    "            gen_standard_aug_weda(train_orig, output_file_weda, num_aug=n_aug)\n",
    "            \n",
    "            output_file_eda = dataset_folder + '/train_eda_'+str(alpha)+'.txt'\n",
    "\n",
    "            gen_standard_aug_eda(train_orig, output_file_eda, num_aug=n_aug)\n",
    "\n",
    "        #generate the vocab dictionary\n",
    "        word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "        gen_vocab_dicts(dataset_folder, word2vec_pickle, huge_word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
