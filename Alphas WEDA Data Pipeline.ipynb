{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c3bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bfdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['1_tiny', '2_small', '3_standard', '4_full']\n",
    "size_folders = ['size_data_f1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'size_data_f1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'size_data_f1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'size_data_f1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'size_data_f1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "if not os.path.isdir('size_data_f1'):\n",
    "    os.mkdir('size_data_f1')\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Number of words for input\n",
    "input_size_list = [50,50,40,25,25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size_folder in size_folders:\n",
    "    if not os.path.isdir(size_folder):\n",
    "        os.mkdir(size_folder)\n",
    "\n",
    "    dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "    n_aug_list = n_aug_list_dict[size_folder]\n",
    "\n",
    "    #for each dataset\n",
    "    for i, dataset_folder in enumerate(dataset_folders):\n",
    "        if not os.path.isdir(dataset_folder):\n",
    "            os.mkdir(dataset_folder)\n",
    "\n",
    "        n_aug = n_aug_list[i]\n",
    "\n",
    "        #pre-existing file locations\n",
    "        train_orig = dataset_folder + '/train_orig.txt'\n",
    "\n",
    "        for alpha in alphas:\n",
    "            output_file = dataset_folder + '/train_weda_'+str(alpha)+'.txt'\n",
    "\n",
    "            gen_standard_aug(train_orig, output_file, alpha, n_aug)\n",
    "\n",
    "        #generate the vocab dictionary\n",
    "        word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "        gen_vocab_dicts(dataset_folder, word2vec_pickle, huge_word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
