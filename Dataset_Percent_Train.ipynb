{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "\n",
    "#percent_data/percent/sst2/train_orig.txt\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['1_tiny', '2_small', '3_standard', '4_full']\n",
    "size_folders = ['size_data_f1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'size_data_f1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'size_data_f1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'size_data_f1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'size_data_f1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "if not os.path.isdir('size_data_f1'):\n",
    "    os.mkdir('size_data_f1')\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "alphas = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Number of words for input\n",
    "input_size_list = [50,50,40,25,25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2929f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_cnn(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "    test_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=20, \n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=1)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y, test_x, test_y, model = None, None, None, None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each method\n",
    "\n",
    "writer_cnn = open('outputs_percentage/cnn_' + a_method + '_' + get_now_str() + '.txt', 'w')\n",
    "\n",
    "\n",
    "#for each size dataset\n",
    "for size_folder in size_folders:\n",
    "\n",
    "    writer_cnn.write(size_folder + '\\n')\n",
    "\n",
    "    #get all six datasets\n",
    "    dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "\n",
    "    #for storing the performances\n",
    "    performances_cnn = {alpha:[] for alpha in alphas}\n",
    "\n",
    "    #for each dataset\n",
    "    for i in range(len(dataset_folders)):\n",
    "\n",
    "        #initialize all the variables\n",
    "        dataset_folder = dataset_folders[i]\n",
    "        dataset = datasets[i]\n",
    "        num_classes = num_classes_list[i]\n",
    "        input_size = input_size_list[i]\n",
    "        word2vec_pickle = dataset_folder + '/word2vec.p'\n",
    "        word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "        #test each alpha value\n",
    "        for alpha in alphas:\n",
    "\n",
    "            train_path = dataset_folder + '/train_weda_' + str(alpha) + '.txt'\n",
    "            test_path = 'size_data_f1/test/' + dataset + '/test.txt'\n",
    "            acc = run_cnn(train_path, test_path, num_classes, percent_dataset=1)\n",
    "            print(\"cnn aug \", acc, alpha, a_method, dataset_folder)\n",
    "\n",
    "            performances_cnn[alpha].append(acc)\n",
    "            \"\"\"\n",
    "            acc = run_rnn(train_path, test_path, num_classes, percent_dataset=1)\n",
    "            performances_rnn[alpha].append(acc)\n",
    "            print(\"rnn aug\", acc)\n",
    "            \"\"\"\n",
    "\n",
    "    writer_cnn.write(str(performances_cnn) + '\\n')\n",
    "    writer_rnn.write(str(performances_rnn)+\"\\n\")\n",
    "    for alpha in performances_cnn:\n",
    "        line = str(alpha) + ' : ' + str(sum(performances_cnn[alpha])/len(performances_cnn[alpha]))\n",
    "        writer_cnn.write(line + '\\n')\n",
    "        print(line)\n",
    "    \"\"\"\n",
    "    for alpha in performances_rnn:\n",
    "        line = str(alpha) + ' : ' + str(sum(performances_rnn[alpha])/len(performances_rnn[alpha]))\n",
    "        writer_rnn.write(line + '\\n')\n",
    "        print(line)\n",
    "    \"\"\"\n",
    "    print(performances_cnn)\n",
    "\n",
    "writer_cnn.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed96f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62a256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaa558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3690ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22812f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e862a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37841db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba488ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
