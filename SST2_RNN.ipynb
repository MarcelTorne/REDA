{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a830370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ace03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user inputs\n",
    "\n",
    "#load hyperparameters\n",
    "sizes = ['4_full']#['1_tiny', '2_small', '3_standard', '4_full']\n",
    "size_folders = ['size_data_t1/' + size for size in sizes]\n",
    "\n",
    "#datasets\n",
    "datasets = ['sst2']\n",
    "\n",
    "#number of output classes\n",
    "num_classes_list = [2, 2, 2, 6, 2]\n",
    "\n",
    "#number of augmentations per original sentence\n",
    "n_aug_list_dict = {'size_data_t1/1_tiny': [32, 32, 32, 32, 32], \n",
    "\t\t\t\t\t'size_data_t1/2_small': [32, 32, 32, 32, 32],\n",
    "\t\t\t\t\t'size_data_t1/3_standard': [16, 16, 16, 16, 4],\n",
    "\t\t\t\t\t'size_data_t1/4_full': [16, 16, 16, 16, 4]}\n",
    "\n",
    "#number of words for input\n",
    "input_size_list = [50, 50, 40, 25, 25]\n",
    "\n",
    "#word2vec dictionary\n",
    "huge_word2vec = 'word2vec/glove.840B.300d.txt'\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3cd7a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished eda for sst2/train_orig.txt to sst2/train_aug_st.txt\n",
      "['sst2/test.txt', 'sst2/train_aug_st.txt', 'sst2/train_orig.txt']\n",
      "33712 unique words found\n",
      "24730 matches between unique words and word2vec dictionary\n",
      "dictionaries outputted to sst2/word2vec.p\n"
     ]
    }
   ],
   "source": [
    "from methods import *\n",
    "\n",
    "\n",
    "for size_folder in size_folders:\n",
    "\n",
    "    dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "    n_aug_list = n_aug_list_dict[size_folder]\n",
    "\n",
    "    #for each dataset\n",
    "    for i, dataset_folder in enumerate(dataset_folders):\n",
    "\n",
    "        n_aug = n_aug_list[i]\n",
    "\n",
    "        #pre-existing file locations\n",
    "        train_orig = 'sst2/train_orig.txt'#dataset_folder + '/train_orig.txt'\n",
    "\n",
    "        #file to be created\n",
    "        train_aug_st = 'sst2/train_aug_st.txt'#dataset_folder + '/train_aug_st.txt'\n",
    "\n",
    "        #standard augmentation\n",
    "        gen_standard_aug(train_orig, train_aug_st, n_aug)\n",
    "\n",
    "        #generate the vocab dictionary\n",
    "        word2vec_pickle = \"sst2\" + '/word2vec.p'\n",
    "        gen_vocab_dicts(\"sst2\", word2vec_pickle, huge_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48a7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(train_file, test_file, num_classes, input_size, percent_dataset, word2vec):\n",
    "\n",
    "    #initialize model\n",
    "    model = build_model(input_size, word2vec_len, num_classes)\n",
    "\n",
    "    #load data\n",
    "    train_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "    test_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "    #implement early stopping\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "    #train model\n",
    "    model.fit(\ttrain_x, \n",
    "                train_y, \n",
    "                epochs=100000, \n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1, \n",
    "                batch_size=1024, \n",
    "                shuffle=True, \n",
    "                verbose=1)\n",
    "    #model.save('checkpoints/lol')\n",
    "    #model = load_model('checkpoints/lol')\n",
    "\n",
    "    #evaluate model\n",
    "    y_pred = model.predict(test_x)\n",
    "    test_y_cat = one_hot_to_categorical(test_y)\n",
    "    y_pred_cat = one_hot_to_categorical(y_pred)\n",
    "    acc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "    #clean memory???\n",
    "    train_x, train_y, model = None, None, None\n",
    "    gc.collect()\n",
    "\n",
    "    #return the accuracy\n",
    "    #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c8c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000\n",
      "111/111 [==============================] - 102s 848ms/step - loss: 0.5669 - accuracy: 0.7000 - val_loss: 0.3934 - val_accuracy: 0.8207\n",
      "Epoch 2/100000\n",
      "111/111 [==============================] - 82s 742ms/step - loss: 0.3641 - accuracy: 0.8382 - val_loss: 0.3056 - val_accuracy: 0.8716\n",
      "Epoch 3/100000\n",
      "111/111 [==============================] - 82s 737ms/step - loss: 0.2641 - accuracy: 0.8909 - val_loss: 0.2456 - val_accuracy: 0.8996\n",
      "Epoch 4/100000\n",
      "111/111 [==============================] - 82s 739ms/step - loss: 0.1835 - accuracy: 0.9299 - val_loss: 0.1914 - val_accuracy: 0.9282\n",
      "Epoch 5/100000\n",
      "111/111 [==============================] - 82s 740ms/step - loss: 0.1307 - accuracy: 0.9522 - val_loss: 0.1392 - val_accuracy: 0.9507\n",
      "Epoch 6/100000\n",
      "111/111 [==============================] - 82s 738ms/step - loss: 0.0871 - accuracy: 0.9691 - val_loss: 0.1165 - val_accuracy: 0.9589\n",
      "Epoch 7/100000\n",
      "111/111 [==============================] - 82s 739ms/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.1169 - val_accuracy: 0.9620\n",
      "Epoch 8/100000\n",
      "111/111 [==============================] - 109s 985ms/step - loss: 0.0595 - accuracy: 0.9794 - val_loss: 0.1047 - val_accuracy: 0.9679\n",
      "Epoch 9/100000\n",
      "111/111 [==============================] - 87s 780ms/step - loss: 0.0419 - accuracy: 0.9856 - val_loss: 0.0918 - val_accuracy: 0.9710\n",
      "Epoch 10/100000\n",
      "111/111 [==============================] - 85s 762ms/step - loss: 0.0434 - accuracy: 0.9849 - val_loss: 0.0938 - val_accuracy: 0.9741\n",
      "Epoch 11/100000\n",
      "111/111 [==============================] - 87s 783ms/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.0937 - val_accuracy: 0.9731\n",
      "Epoch 12/100000\n",
      "111/111 [==============================] - 85s 768ms/step - loss: 0.0371 - accuracy: 0.9872 - val_loss: 0.0910 - val_accuracy: 0.9755\n",
      "Epoch 13/100000\n",
      "111/111 [==============================] - 83s 749ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.1229 - val_accuracy: 0.9680\n",
      "Epoch 14/100000\n",
      "111/111 [==============================] - 88s 798ms/step - loss: 0.0174 - accuracy: 0.9942 - val_loss: 0.0894 - val_accuracy: 0.9761\n",
      "Epoch 15/100000\n",
      "111/111 [==============================] - 89s 801ms/step - loss: 0.0194 - accuracy: 0.9934 - val_loss: 0.1384 - val_accuracy: 0.9671\n",
      "Epoch 16/100000\n",
      "111/111 [==============================] - 84s 752ms/step - loss: 0.0236 - accuracy: 0.9918 - val_loss: 0.0853 - val_accuracy: 0.9772\n",
      "Epoch 17/100000\n",
      "111/111 [==============================] - 85s 766ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0980 - val_accuracy: 0.9770\n",
      "Epoch 18/100000\n",
      "111/111 [==============================] - 86s 778ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.0963 - val_accuracy: 0.9778\n",
      "Epoch 19/100000\n",
      "111/111 [==============================] - 88s 795ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.1068 - val_accuracy: 0.9751\n",
      "0.8130360205831904\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### get baseline accuracies ###\n",
    "###############################\n",
    "\n",
    "def compute_baselines_aug(writer):\n",
    "\n",
    "    #baseline computation\n",
    "    for size_folder in size_folders:\n",
    "\n",
    "        #get all six datasets\n",
    "        dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "        performances = []\n",
    "\n",
    "        #for each dataset\n",
    "        for i in range(len(dataset_folders)):\n",
    "\n",
    "            #initialize all the variables\n",
    "            dataset_folder = dataset_folders[i]\n",
    "            dataset = datasets[i]\n",
    "            num_classes = num_classes_list[i]\n",
    "            input_size = input_size_list[i]\n",
    "            word2vec_pickle =  'sst2/word2vec.p'\n",
    "            word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "            train_path = 'sst2/train_aug_st.txt'\n",
    "            test_path =  dataset + '/test.txt'\n",
    "            acc = run_rnn(train_path, test_path, num_classes, input_size, 1, word2vec)\n",
    "            performances.append(str(acc))\n",
    "\n",
    "        line = ','.join(performances)\n",
    "        print(line)\n",
    "        writer.write(line+'\\n')\n",
    "\n",
    "###############################\n",
    "############ main #############\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "writer = open('baseline_aug_rnn/' + get_now_str() + '.csv', 'w')\n",
    "\n",
    "\"\"\"\n",
    "for i in range(0, 10):\n",
    "\n",
    "    seed(i)\n",
    "    print(i)\n",
    "\"\"\"\n",
    "compute_baselines_aug(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e879d77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sst2/train_orig_st.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-94f8c6ccee64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \"\"\"\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mcompute_baselines_orig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-94f8c6ccee64>\u001b[0m in \u001b[0;36mcompute_baselines_orig\u001b[0;34m(writer)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sst2/train_orig_st.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mperformances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-24a1bf3c16a0>\u001b[0m in \u001b[0;36mrun_rnn\u001b[0;34m(train_file, test_file, num_classes, input_size, percent_dataset, word2vec)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NLP project/methods.py\u001b[0m in \u001b[0;36mget_x_y\u001b[0;34m(train_txt, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m#read in lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mtrain_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtrain_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercent_dataset\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sst2/train_orig_st.txt'"
     ]
    }
   ],
   "source": [
    "def compute_baselines_orig(writer):\n",
    "\n",
    "    #baseline computation\n",
    "    for size_folder in size_folders:\n",
    "\n",
    "        #get all six datasets\n",
    "        dataset_folders = [size_folder + '/' + s for s in datasets]\n",
    "        performances = []\n",
    "\n",
    "        #for each dataset\n",
    "        for i in range(len(dataset_folders)):\n",
    "\n",
    "            #initialize all the variables\n",
    "            dataset_folder = dataset_folders[i]\n",
    "            dataset = datasets[i]\n",
    "            num_classes = num_classes_list[i]\n",
    "            input_size = input_size_list[i]\n",
    "            word2vec_pickle =  'sst2/word2vec.p'\n",
    "            word2vec = load_pickle(word2vec_pickle)\n",
    "\n",
    "            train_path = 'sst2/train_orig.txt'\n",
    "            test_path =  dataset + '/test.txt'\n",
    "            acc = run_rnn(train_path, test_path, num_classes, input_size, 1, word2vec)\n",
    "            performances.append(str(acc))\n",
    "\n",
    "        line = ','.join(performances)\n",
    "        print(line)\n",
    "        writer.write(line+'\\n')\n",
    "\n",
    "###############################\n",
    "############ main #############\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "writer = open('baseline_orig_rnn/' + get_now_str() + '.csv', 'w')\n",
    "\n",
    "\"\"\"\n",
    "for i in range(0, 10):\n",
    "\n",
    "    seed(i)\n",
    "    print(i)\n",
    "\"\"\"\n",
    "compute_baselines_orig(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fe9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
